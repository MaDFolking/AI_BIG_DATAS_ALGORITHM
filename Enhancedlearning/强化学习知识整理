相关知识网址: https://zhuanlan.zhihu.com/p/24446336
增强学习原理(里面有动态规划求解:) : https://zhuanlan.zhihu.com/p/25319023

一.基本概念
包含:
智能体(agent):操控的东西，比如计算机，机器人。
状态(state): 当前的状态。
行为(action) :根据当前的状态，进行下一步的行为。 
奖励(reward):对于你的行为产生的结果进行奖励/惩罚。
也就是有个衡量你做的对错。
策略(policy):按照一系列的行为组合最终能得到最大的奖励值。

遵循原则:先观察，再行动，再观测。
每一个动作都能影响代理将来的状态，通过一个标量的奖励信号衡量成功。
目标:选择一系列行为来最大化未来的奖励。


二.马尔科夫决策要求
1.能够检测到理想的状态

2.可以多次尝试

3.系统的下个状态只与当前状态信息有关,而与更早之前的状态无关,在决策过程中还和当前采取的动作有关。

4.由5个元素组成
(1) S: 表示状态集(states)
(2) A: 表示一组动作(actions)
(3) P: 表示状态转移概率Psa表示当前s∈S状态下,经过a∈A作用后,会转移到其他状态的概率分布情况
在状态s下执行动作a,转移到s'的概率可以表示p(s'|s,a)
(4) R: 奖励函数(reward function) 表示agent采取某个动作后的即时奖励
(5) γ: 折扣系数意味着当下的reward比未来反馈的reward更重要。取0~1 左闭右开。

6.Q-learning
入门: https://www.cnblogs.com/xiaoxuebiye/p/7753772.html
http://www.infoq.com/cn/articles/painless-enhanced-learning-portal-part01
进阶: http://www.infoq.com/cn/articles/painless-enhanced-learning-portal-part02
策略迭代: http://www.infoq.com/cn/articles/painless-enhanced-learning-portal-part03
价值迭代: http://www.infoq.com/cn/articles/painless-enhanced-learning-portal-part04
泛化迭代: http://www.infoq.com/cn/articles/painless-enhanced-learning-portal-part05
大纲: https://www.toutiao.com/a6557206054529139213/
蒙特卡罗法与增强学习: http://www.infoq.com/cn/articles/painless-enhanced-learning-portal-part06
差分时序法:
http://www.infoq.com/cn/articles/painless-enhanced-learning-portal-part07

核心思想:
先把方案化成图，再根据图做出矩阵



最后是计算矩阵各个之间数值，得到各个权值，再找到权值最大的路线，最后这个路线跟图论算法很像。


五. DQN: Deep Q Network(强化学习加入到神经网络)

http://www.cnblogs.com/cjnmy36723/p/7017549.html
https://blog.csdn.net/gongxiaojiu/article/details/73345808
https://blog.csdn.net/u010159842/article/details/72782309
强化学习迷宫案例: https://github.com/leoChaoGlut/machine-learning-practice/tree/master/reinforcement-learning/q-learning/maze
强化学习解决迷宫问题: http://www.sohu.com/a/134786263_723464
解决迷宫代码:https://cn.aliyun.com/jiaocheng/517215.html
高级篇:http://sports.sina.com.cn/go/2017-12-07/doc-ifyppemf5737801.shtml
小鸟项目:
每次都找到最优解，但不一定是全局最优，这是贪心法。
探索和开发都具备，有机会就要探索=》加入贪心算法。
加入一种概率:90%是普通贪心算法，10%去探索开发。==》小鸟游戏。
神经网路中也有当前价值和实际奖励。
