一.Linux
1.配置:3G内存，20G硬盘,Native模式

2.修改主机名 hostname:查看主机名
          hostname   host_name  ：临时修改主机名
3.永久修改:  vim  /etc/sysconfig/network
          cat /etc/sysconfig/network 
4.重启centos
5.IP、子网掩码、网关、DNS		
这里需要注意，我们的子网与DNS最好一样，点击虚拟机左上角的编辑里的虚拟网络编辑可以查到，在nat选项里，
里面也可以查到我们能用到的IP地址范围，必须遵守这个范围，否则会出错。	
6.重启网络服务
查看ip地址：ifconfig
service  network  restart
7.修改配置文件  /etc/sysconfig/network-scripts/ifcfg-eth0
主机名与IP的映射：/etc/hosts

添加一条
192.168.134.200	hadoop-senior01.ibeifeng.com
		
配置windows的本地映射
C:\Windows\System32\drivers\etc\hosts
		
弄完了别忘了 ping 192.168.134.200  或者 hadoop-senior01.ibeifeng.com

8.关闭防火墙和selinux
  service iptables stop
  chkconfig  iptables  off
  
  注意:
  chkconfig用于很多命令的开机启动，比如mysql的:
  chkconfig mysqld on
  
  vim  /etc/selinux/config
  SELINUX=disabled

9.sudo的配置
	-》应用：临时获取管理员权限执行某些操作
	-》配置sudo：visudo:只有root用户才能执行
		root    ALL=(ALL)       ALL
		表root用户可以通过任何机器以任何身份执行任何命令
		用户	主机名=（以什么用户的身份执行）	可以执行的命令
		beifeng	ALL=(root)	/sbin/service iptables status
		使用：
			sudo command
		beifeng ALL=(root)      /sbin/service iptables status
		beifeng ALL=(root)      NOPASSWD:/sbin/service iptables start
	-》beifeng获取所有权限
		beifeng	ALL=(root)		NOPASSWD:ALL
	-》相关文档
		man sudoers 5

10.Linux磁盘管理
	RAID
	hadoop不需要做RAID
	磁盘类型
	IDE
		hda,hdb
	SCSI：s	
		sda:第一个磁盘（sda1是第一个主分区，sda2第二个主分区... 最多四个
		
		sdb:第二个磁盘
		分区：/dev/sda1  /dev/sda2 /dev/sda3 /dev/sda4
		在CentOS系统中本地硬盘识别为/dev/sda，再把U盘挂载上去后就会识别为/dev/sdb
			
	SATA
	SSD
	相关命令
		fdisk:管理磁盘的命令
		-l:查看当前系统的磁盘分区
		Linux上的存储
		磁盘-》分区-》格式化（得到文件系统类型）-》文件夹
		Linux中分区：2+1,3+1
		主分区
		扩展分区：
		创建逻辑分区
			sda5,sda6……
		逻辑卷组（pvcreate,vgcreate,lvcreate       lvextend,resize2fs）
			pvcreate：动态逻辑卷分区
			vg:动态逻辑卷组
			lv:动态逻辑卷
			主分区+扩展分区《=4
		用于查看当前磁盘利用率
			df -h
	-》添加硬盘，创建分区
		-》添加硬盘
			硬盘进行分区
			fdisk dev_path
			fdisk /dev/sdb
			新建主分区
				新建分区
				格式化
				mkfs -t ext4  /dev/sdb1
				挂载
				mount -l 
				mount /dev/sdb1  /mnt/primary
				重新启动以后还有没有？
			新建扩展分区
				创建分区
				格式化
				mkfs -t ext4  /dev/sdb5
				挂载测试
				mount /dev/sdb5  /mnt/logic
			实现永久挂载
				vim  /etc/fstab
				/dev/sdb1  /mnt/primary     ext4    defaults        0 0
				/dev/sdb5  /mnt/logic       ext4    defaults        0 0
			重启检查
			
Linux中的软件管理
	-》软件的类型
		-》源码
			-》底层用C、C++编写的 
				-》./configure    :预编译
				-》make			  ：编译
				-》make install	  ：安装
			-》底层用Java编写的
				-》maven
		-》二进制
	-》rpm包
		-》rpm
			zlib-1.2.3-29.el6.i686.rpm
			软件的名称-版本-操作系统-操作系统的位数.rpm
		-》查看已安装包
			rpm  -qa | grep java
		-》安装rpm包
			rpm  -ivh  package_path
			rpm -ivh Packages/zlib-devel-1.2.3-29.el6.x86_64.rpm 
		-》卸载
			rpm  -e   package_name
			rpm -e zlib-devel-1.2.3-29.el6.x86_64
			rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64
			不考虑依赖强制删除
		-》依赖问题：
			rpm -ivh Packages/zlib-devel-1.2.3-29.el6.i686.rpm
		-》查看某个命令来自哪个包
			whereis man
			rpm -qf /usr/bin/man
			rpm -qf  command_path
		-》查看某个包装了哪些文件
			rpm -ql  package_name
			rpm -ql tzdata-java-2013g-1.el6.noarch
	-》yum 安装
		-》本地必须要有yum源
			ll /etc/yum.repos.d/
		-》系统可以联网
		-》查看哪些包可以安装
			yum list
			yum list installed
		-》安装
			yum install -y name1  name2 
			yum install  -y  name
		-》删除
			yum remove  -y name
	-》安装JDK
		-》包上传到Linux
		-》tar解压到/opt/modules/
		-》全局设置JAVA_HOME
			vim /etc/profile
			export  JAVA_HOME=/opt/modules/java-
			export  PATH=$PATH:$JAVA_HOME/bin
		-》source  /etc/profile
		-》java -version
		

二.mysql和tcal包:
创建好mysql后,需要设置用户权限
1.create user 'root' identified by 'root';
2.给root授权,这是用本地进行访问时给root用户的权限
grant all on *.* to root@localhost identified by 'root';
3.同理也得给master授权
grant all on *.* to root@master identified by 'root';
4.创建数据库
create database mahout;

5.解压tcl包，需要有gcc编译器
  yum install gcc
  yum install gcc-c++

三.安装redis
redis 需要的内存是比较大的，只有类型列表，没有表。一定程度上也可以持久化到磁盘上。
必须在root用户下做
进入/home/master/tcl8.6.1 也就是在tcl目录下。

输入:
cd unix &&
./configure --prefix=/usr \
--mandir=/usr/share/man \
--without-tzdata \
$([ $(uname -m) = x86_64 ] && echo --enable-64bit) &&
make &&
sed -e "s@^\(TCL_SRC_DIR='\).*@\1/usr/include'@" \
-e "/TCL_B/s@='\(-L\)\?.*unix@='\1/usr/lib@" \
-i tclConfig.sh
make install &&
make install-private-headers &&
ln -v -sf tclsh8.6 /usr/bin/tclsh &&
chmod -v 755 /usr/lib/libtcl8.6.so

top查看虚拟内存

下面是正式安装redis

解压后进入redis目录，编译:
make -j 4
编译好后，cd src
make test
回到redis目录，输入
make install
然后启动redis,因为我们每次都用到，让它启动在后台。
nohup redis-server &
完事后输入
tail -f nohup.out
查看日志
也可以查看进程
ps -ef|grep redis-server
每次启动redis客户端前，要输入redis-server 启动服务端，再克隆一个界面启动客户端
进入客户端
redis-cli
输入ping 如果显示pong就是启动成功了。

四.安装Kafka
这里使用kafka自带的zookeeper集群
进入kafka后
cd config
可以查看zookeeper信息
便于操作可以配置bash_profile环境

#JAVA
export JAVA_HOME=/home/master/software/jdk1.7.0_71
export PATH=$PATH:$JAVA_HOME/bin

#HADOOP
export HADOOP_HOME=/home/master/software/hadoop-2.6.5
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

#MYSQL
export MYSQL_HOME=/home/master/software/mysql
export PATH=$PATH:$MYSQL_HOME/bin

#KAFKA
export KAFKA_HOME=/home/master/software/kafka_2.10-0.8.2.2
export PATH=$PATH:$KAFKA_HOME/bin

#SPARK_HOME
export SPARK_HOME=/home/master/software/spark-1.6.2-bin-hadoop2.6
export PATH=$PATH:$SPARK_HOME/bin

因为也是开机启动zookeeper,所以也用后台方式启动，回到kafka目录，输入
nohup bin/zookeeper-server-start.sh config/zookeeper.properties &
再输入
tail -F nohup.out
查看日志
开启broker
在config/server.propers 查看信息
启动:
 nohup bin/kafka-server-start.sh config/server.properties &
查看日志信息
tail -F nohup.out  

五.安装spark
设置好后，开启命令
进入spark目录
运行spark:
spark-submit --class org.apache.spark.examples.SparkPi \
--master yarn-client \
--num-executors 3 \
--driver-memory 1g \
--executor-memory 1g \
--executor-cores 1 \
lib/spark-examples*.jar \
10

spark-shell --master local[2]

六 . 配置hbase文件
vi hbase-site.xml
<configuration>
//设置hbase客户端启动
 <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
 </property>
//设置hbase存放目录，在hdfs创建个/hbase文件目录
 <property>
        <name>hbase.root.dir</name>
        <value>/hbase</value>
 </property>
</configuration>

vi hbase-env.sh
ESC /ZOOKEEPER 这是Citrl+F操作，找到zookeeper看是否配置，我们直接用自带即可。

start-hbase.sh 打开进程看看。
会发现给出提示 JAVA_HOME is not 设置。
去设置java

hbase shell
出错:
java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected
解决
问题原因：/usr/java/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar 版本低了
解决办法： 
rm -rf  /usr/java/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar
cp /usr/java/apache-hive-2.1.1-bin/lib/jline-2.12.jar /usr/java/hadoop-2.6.5/share/hadoop/yarn/lib/
